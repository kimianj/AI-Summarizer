{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165ed33-80e6-481f-ba31-e19e883062dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27773398-3861-4dad-a097-c724a4906bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "subset_size = 10000\n",
    "trainFile = pd.read_csv('train.csv')\n",
    "testFile = pd.read_csv('test.csv')\n",
    "\n",
    "sampled_train = trainFile.sample(n=subset_size, random_state=40)\n",
    "sampled_test = testFile.sample(n=subset_size, random_state=40)\n",
    "\n",
    "train_articles = sampled_train['article'] \n",
    "train_summaries = sampled_train['highlights']  \n",
    "\n",
    "test_articles =sampled_test['article']   \n",
    "test_summaries = sampled_test['highlights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b02f7-cacb-44a9-93d7-1f8ee1ddef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.!?]', '', text)  # Preserve punctuation for sentence splitting\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "sampled_train['article'] = sampled_train['article'].apply(clean_text)\n",
    "sampled_train['highlights'] = sampled_train['highlights'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2d8f3-0085-4e56-b8a6-537b7cfb0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n sentences of an article\n",
    "def get_lead_by_word_count(text, word_limit=100):\n",
    "    sentences = text.split('. ')\n",
    "    selected_sentences = []\n",
    "    current_word_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_word_count += len(sentence.split())\n",
    "        if current_word_count <= word_limit:\n",
    "            selected_sentences.append(sentence)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return '. '.join(selected_sentences)\n",
    "\n",
    "sampled_train['predicted_summary'] = sampled_train['article'].apply(get_lead_by_word_count)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Original Summary:\\n\", sampled_train['article'].iloc[i])\n",
    "    print(\"\\n\")\n",
    "    print(\"Predicted Summary:\\n\", sampled_train['predicted_summary'].iloc[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd55d38-81a5-4054-a72f-557d821e821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF scores\n",
    "def tfidf_based_summarization(text, n=5):\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    vectorizer = TfidfVectorizer().fit(sentences)\n",
    "    tfidf_matrix = vectorizer.transform(sentences)\n",
    "    sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
    "    \n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(sentence_scores, axis=0)[-n:]]\n",
    "    return '. '.join(ranked_sentences)\n",
    "\n",
    "sampled_train['predicted_summary'] = sampled_train['article'].apply(tfidf_based_summarization)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Original Summary:\\n\", sampled_train['article'].iloc[i])\n",
    "    print(\"\\n\")\n",
    "    print(\"Predicted Summary:\\n\", sampled_train['predicted_summary'].iloc[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724bf5e-931d-4cc3-925e-11c825e46aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def rouge_1_score(summary, reference):\n",
    "    \"\"\"Calculate ROUGE-1 (unigram overlap) score between summary and reference.\"\"\"\n",
    "    summary_unigrams = Counter(summary.split())\n",
    "    reference_unigrams = Counter(reference.split())\n",
    "    \n",
    "    # Find the number of unigram overlaps\n",
    "    overlap = sum(min(count, reference_unigrams[word]) for word, count in summary_unigrams.items())\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    precision = overlap / sum(summary_unigrams.values())\n",
    "    recall = overlap / sum(reference_unigrams.values())\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "# Assuming sampled_train['reference_summary'] contains the gold standard summaries\n",
    "sampled_train['rouge_1_score'] = sampled_train.apply(lambda row: rouge_1_score(row['predicted_summary'], row['highlights']), axis=1)\n",
    "\n",
    "# Calculate the average ROUGE-1 score across all summaries\n",
    "average_rouge_1 = sampled_train['rouge_1_score'].mean()\n",
    "print(\"Average ROUGE-1 Score:\", average_rouge_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738469d-996d-4018-8d65-c964158ecc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Pre-fit the TF-IDF vectorizer on all sentences\n",
    "all_sentences = [sentence for article in sampled_train['article'] for sentence in article.split('. ')]\n",
    "vectorizer = TfidfVectorizer().fit(all_sentences)\n",
    "\n",
    "def apply_clustering(tfidf_matrix, num_clusters=5):\n",
    "    # Use MiniBatchKMeans for faster processing\n",
    "    km = MiniBatchKMeans(n_clusters=num_clusters, batch_size=1000, n_init=10)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_\n",
    "    return clusters\n",
    "\n",
    "def tfidf_based_summarization_with_clustering(text, n=5):\n",
    "    sentences = text.split('. ')\n",
    "    num_clusters = min(n, len(sentences)) if len(sentences) > 1 else 1\n",
    "    tfidf_matrix = vectorizer.transform(sentences)\n",
    "    \n",
    "    if num_clusters > 1:\n",
    "        sentence_clusters = apply_clustering(tfidf_matrix, num_clusters=num_clusters)\n",
    "        clustered_sentences = []\n",
    "        for cluster_num in range(num_clusters):\n",
    "            cluster_indices = np.where(sentence_clusters == cluster_num)[0]\n",
    "            if cluster_indices.size == 0:\n",
    "                continue\n",
    "            cluster_scores = tfidf_matrix[cluster_indices].sum(axis=1).A1\n",
    "            highest_score_index = cluster_indices[np.argmax(cluster_scores)]\n",
    "            clustered_sentences.append(sentences[highest_score_index])\n",
    "        return '. '.join(clustered_sentences)\n",
    "    else:\n",
    "        return text if sentences else ''\n",
    "\n",
    "# Process the data\n",
    "clustered_summaries = [tfidf_based_summarization_with_clustering(article, n=5) for article in sampled_train['article']]\n",
    "sampled_train['clustered_summary'] = clustered_summaries\n",
    "\n",
    "# Display the results for the first few articles\n",
    "for i in range(5):\n",
    "    print(\"Original Summary:\\n\", sampled_train['article'].iloc[i])\n",
    "    print(\"\\n\")\n",
    "    print(\"Clustered Summary:\\n\", sampled_train['clustered_summary'].iloc[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0264da-59af-439a-952d-8dd1be183796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "def calculate_rouge_scores(clustered_summaries, reference_summaries):\n",
    "    rouge = Rouge()\n",
    "    scores = [rouge.get_scores(cs, rs) for cs, rs in zip(clustered_summaries, reference_summaries)]\n",
    "    return scores\n",
    "\n",
    "rouge_scores = calculate_rouge_scores(sampled_train['clustered_summary'], sampled_train['highlights'])\n",
    "print(rouge_scores[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c83c86-8c89-4195-8b15-38f7a760708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the parameters of the last layer\n",
    "for param in model.model.decoder.layers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "inputs = tokenizer([\"article\"], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "labels = tokenizer([\"predicted_summary\"], return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(inputs.input_ids, labels.input_ids)\n",
    "data_loader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# optimize the parameters (unfrozen)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "model.train()\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0]\n",
    "        labels = batch[1]\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ee8c3-a054-4b44-8de4-9e941642bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train.reset_index(drop=True, inplace=True)\n",
    "sampled_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextSummarizationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_input_length=512, max_output_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.dataframe.iloc[idx]['article']\n",
    "        summary = self.dataframe.iloc[idx]['highlights']\n",
    "        inputs = self.tokenizer(article, return_tensors=\"pt\", max_length=self.max_input_length, truncation=True, padding=\"max_length\")\n",
    "        outputs = self.tokenizer(summary, return_tensors=\"pt\", max_length=self.max_output_length, truncation=True, padding=\"max_length\")\n",
    "        return inputs.input_ids.squeeze(0), outputs.input_ids.squeeze(0)\n",
    "\n",
    "train_dataset = TextSummarizationDataset(sampled_train, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "model.train()\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (input_ids, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2062eae-312f-4e15-b019-afb152701e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Costumized dataset for better standardization and flexibility\n",
    "from torch.utils.data import Dataset\n",
    "from torch import tensor\n",
    "\n",
    "class CNNDailyMailDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, tokenizer, max_length=512):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        if idx >= len(self.articles):\n",
    "            raise IndexError(f\"Index {idx} out of range\")\n",
    "\n",
    "        # Tokenize the article and summary\n",
    "        input_ids = self.tokenizer.encode(article, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\").squeeze()\n",
    "        label_ids = self.tokenizer.encode(summary, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\").squeeze()\n",
    "\n",
    "        return input_ids, label_ids\n",
    "\n",
    "\n",
    "train_articles.reset_index(drop=True, inplace=True)\n",
    "train_summaries.reset_index(drop=True, inplace=True)\n",
    "test_articles.reset_index(drop=True, inplace=True)\n",
    "test_summaries.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Then create the datasets\n",
    "train_dataset = CNNDailyMailDataset(train_articles, train_summaries, tokenizer)\n",
    "test_dataset = CNNDailyMailDataset(test_articles, test_summaries, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 1020:\n",
    "        print(f\"Batch at index 1020: {batch}\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf27733-16e9-4227-b6ec-8c1fc8093881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the parameters of the last layer in the decoder\n",
    "for param in model.model.decoder.layers[-1].parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf45818-ae8f-4823-8b73-f54c766e561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 1020 error, this code helped me to get the better idea of where the error might come from.\n",
    "print(len(train_articles), len(train_summaries))\n",
    "train_articles.reset_index(drop=True, inplace=True)\n",
    "train_summaries.reset_index(drop=True, inplace=True)\n",
    "for i, batch in enumerate(train_loader):\n",
    "    try:\n",
    "        input_ids, labels = batch\n",
    "    except KeyError as e:\n",
    "        print(f\"Error at batch index: {i}\")\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4635d-c56f-45b1-9b00-f4fbdacac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.eval()  \n",
    "\n",
    "test_article = sampled_test['article'].iloc[0]\n",
    "inputs = tokenizer([test_article], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(generated_summary)\n",
    "\n",
    "for i in range(5):\n",
    "    test_article = sampled_test['article'].iloc[i]\n",
    "    inputs = tokenizer([test_article], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Article {i+1} Summary:\")\n",
    "    print(generated_summary)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60d69c-9f20-49f4-92e8-008ec389a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge Test\n",
    "reference_summaries = [sampled_test['highlights'].iloc[i] for i in range(5)]\n",
    "generated_summaries = []\n",
    "for i in range(5):\n",
    "    test_article = sampled_test['article'].iloc[i]\n",
    "    inputs = tokenizer([test_article], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    generated_summaries.append(generated_summary)\n",
    "from rouge import Rouge \n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
    "\n",
    "print(\"Rouge Scores:\", rouge_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1c8fd-da3c-4f68-8b49-6df065331d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract F1-scores, Precision, and Recall for each ROUGE type\n",
    "rouge_f1_scores = [rouge_scores['rouge-1']['f'], rouge_scores['rouge-2']['f'], rouge_scores['rouge-l']['f']]\n",
    "rouge_precision_scores = [rouge_scores['rouge-1']['p'], rouge_scores['rouge-2']['p'], rouge_scores['rouge-l']['p']]\n",
    "rouge_recall_scores = [rouge_scores['rouge-1']['r'], rouge_scores['rouge-2']['r'], rouge_scores['rouge-l']['r']]\n",
    "\n",
    "rouge_types = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "\n",
    "print(rouge_types)\n",
    "print(rouge_f1_scores)\n",
    "print(rouge_precision_scores)\n",
    "print(rouge_recall_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081f624-05c0-4768-bcca-e0e02b2b1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the positions and width for the bars\n",
    "positions = range(len(rouge_types))\n",
    "width = 0.2\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([p - width for p in positions], rouge_f1_scores, width, alpha=0.7, label='F1 Score')\n",
    "plt.bar(positions, rouge_precision_scores, width, alpha=0.7, label='Precision')\n",
    "plt.bar([p + width for p in positions], rouge_recall_scores, width, alpha=0.7, label='Recall')\n",
    "\n",
    "plt.xlabel('ROUGE Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('ROUGE Score Visualization')\n",
    "plt.xticks(positions, rouge_types)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b3670-2749-4582-ba63-46789a81f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n=2):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    # Create n-grams\n",
    "    n_grams = zip(*[words[i:] for i in range(n)])\n",
    "    return [' '.join(n_gram) for n_gram in n_grams]\n",
    "\n",
    "generated_ngrams = get_ngrams(generated_summary, n=1)\n",
    "reference_ngrams = get_ngrams(reference_summary, n=1)\n",
    "def overlap_matrix(generated_ngrams, reference_ngrams):\n",
    "    matrix = np.zeros((len(generated_ngrams), len(reference_ngrams)))\n",
    "    for i, g_ngram in enumerate(generated_ngrams):\n",
    "        for j, r_ngram in enumerate(reference_ngrams):\n",
    "            matrix[i][j] = 1 if g_ngram == r_ngram else 0\n",
    "    return matrix\n",
    "\n",
    "overlap_matrix = overlap_matrix(generated_ngrams, reference_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d0150-12a7-4ee3-81d5-5c540c5d148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'overlap_matrix' is already computed and is a large matrix\n",
    "# Limit the number of n-grams to the top N most frequent\n",
    "\n",
    "# Define the number of n-grams to display\n",
    "top_n = 30\n",
    "\n",
    "# Find the top n most frequent n-grams in each summary\n",
    "generated_top_ngrams = np.argsort(-np.sum(overlap_matrix, axis=1))[:top_n]\n",
    "reference_top_ngrams = np.argsort(-np.sum(overlap_matrix, axis=0))[:top_n]\n",
    "\n",
    "# Create a reduced overlap matrix\n",
    "reduced_matrix = overlap_matrix[generated_top_ngrams, :][:, reference_top_ngrams]\n",
    "\n",
    "# Plot the reduced heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(reduced_matrix, annot=False, cmap='Blues')\n",
    "plt.title(\"Reduced N-gram Overlap between Generated and Reference Summary\")\n",
    "plt.xlabel(\"Top Reference Summary N-grams\")\n",
    "plt.ylabel(\"Top Generated Summary N-grams\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788e2ac-4c88-4cf9-8f69-c8d14dfef2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Join all the generated summaries into one big text\n",
    "all_generated_summaries = ' '.join(sampled_train['predicted_summary'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_generated_summaries)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Generated Summaries')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d32506-bb41-4bd3-8c01-3cdfbfd22830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate the length of each summary\n",
    "generated_lengths = sampled_train['predicted_summary'].apply(len)\n",
    "reference_lengths = sampled_train['highlights'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(generated_lengths, color='blue', label='Generated Summaries', kde=True)\n",
    "sns.histplot(reference_lengths, color='red', label='Reference Summaries', kde=True)\n",
    "plt.legend()\n",
    "plt.title('Length Distribution of Summaries')\n",
    "plt.xlabel('Length of Summaries')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006494a8-acbc-4b0a-b79d-754a5e119959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with ROUGE scores\n",
    "\n",
    "# Extract F1-scores, Precision, and Recall for each ROUGE type\n",
    "rouge_f1_scores = [rouge_scores['rouge-1']['f'], rouge_scores['rouge-2']['f'], rouge_scores['rouge-l']['f']]\n",
    "rouge_precision_scores = [rouge_scores['rouge-1']['p'], rouge_scores['rouge-2']['p'], rouge_scores['rouge-l']['p']]\n",
    "rouge_recall_scores = [rouge_scores['rouge-1']['r'], rouge_scores['rouge-2']['r'], rouge_scores['rouge-l']['r']]\n",
    "\n",
    "rouge_types = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "\n",
    "\n",
    "rouge_scores_df = pd.DataFrame({\n",
    "    'rouge-1': [ 0.4200915032679739,  0.43524261927962116, 0.41843497412610464],  # Replace with actual scores\n",
    "    'rouge-2': [0.1577885411202425, 0.1801897684216613, 0.16296273877022555],\n",
    "    'rouge-l': [0.4053202614379085,0.41702630049404243,0.40215454569377745]\n",
    "})\n",
    "\n",
    "sns.boxplot(data=rouge_scores_df)\n",
    "plt.title('ROUGE Scores Distribution')\n",
    "plt.ylabel('Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb865c1-f734-464b-ab66-92b60e1eb5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'articles' is a list of all news articles in your dataset\n",
    "# For example: articles = [article1_text, article2_text, ...]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train_articles)\n",
    "cumulative_features = []\n",
    "feature_count = 0\n",
    "for i in range(X.shape[0]):\n",
    "    feature_count = len(vectorizer.get_feature_names_out()[:i+1])\n",
    "    cumulative_features.append(feature_count)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_features, label='Cumulative Features')\n",
    "plt.xlabel('Number of Articles')\n",
    "plt.ylabel('Cumulative Number of Features')\n",
    "plt.title('Number of Features vs Total Number of Data Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74104e-b0dc-4703-b1d6-e3776634813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Set up the data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "# Define the optimizer\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Adjust based on the actual structure of your batch\n",
    "        input_ids, labels = batch\n",
    "\n",
    "        # Assuming the attention mask is not provided and needs to be created\n",
    "        attention_mask = torch.ones_like(input_ids).to(input_ids.device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_model')\n",
    "\n",
    "# Evaluate with ROUGE after generating summaries on validation data\n",
    "# ... [Code to generate summaries and calculate ROUGE scores] ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e138ff-3415-42e2-bb82-0a7840419f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def clean_summary(summary):\n",
    "    sentences = sent_tokenize(summary)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "# Initialize the grammar checker\n",
    "\n",
    "def post_process(summary):\n",
    "    # Grammar and punctuation correction\n",
    "    corrected_summary = tool.correct(summary)\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(corrected_summary)\n",
    "\n",
    "    # Remove any repeated sentences\n",
    "    unique_sentences = []\n",
    "    seen = set()\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "\n",
    "    # Reassemble the summary\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "for i in range(5):\n",
    "    test_article = sampled_test['article'].iloc[i]\n",
    "    inputs = tokenizer([test_article], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Article {i+1} Summary:\")\n",
    "    print(generated_summary)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4c41b-89a7-436d-a100-8bad3bc25401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "model.to('cpu')  # Run on CPU\n",
    "\n",
    "# Function to generate summaries for a batch of articles\n",
    "def generate_summaries(articles):\n",
    "    summaries = []\n",
    "    for article in articles:\n",
    "        inputs = tokenizer([article], return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\").to('cpu')\n",
    "        summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(generated_summary)\n",
    "    return summaries\n",
    "\n",
    "# Generate summaries in batches\n",
    "batch_size = 10  # Adjust based on your GPU's memory\n",
    "generated_summaries = []\n",
    "for i in range(0, len(sampled_test), batch_size):\n",
    "    batch_articles = sampled_test['article'].iloc[i:i + batch_size].tolist()\n",
    "    batch_summaries = generate_summaries(batch_articles)\n",
    "    generated_summaries.extend(batch_summaries)\n",
    "\n",
    "# Post-process your generated summaries\n",
    "post_processed_summaries = [post_process(clean_summary(summary)) for summary in generated_summaries]\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = [scorer.score(ref_summary, gen_summary) for ref_summary, gen_summary in zip(reference_summaries, post_processed_summaries)]\n",
    "\n",
    "# Compute the average ROUGE scores\n",
    "avg_rouge_scores = {key: np.mean([score[key].fmeasure for score in rouge_scores]) for key in rouge_scores[0]}\n",
    "\n",
    "print(\"Average ROUGE scores after post-processing:\", avg_rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017673e-4eb4-4174-bf45-bd4930a8c713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
